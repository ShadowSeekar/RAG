{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_kgQ0-qPu4Uq",
        "outputId": "e18abf1b-0b05-4db5-ccad-111d97edc7c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (5.0.1)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.12.14)\n",
            "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
            "  Using cached pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\n",
            "Using cached pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
            "Installing collected packages: pinecone-plugin-inference\n",
            "  Attempting uninstall: pinecone-plugin-inference\n",
            "    Found existing installation: pinecone-plugin-inference 3.1.0\n",
            "    Uninstalling pinecone-plugin-inference-3.1.0:\n",
            "      Successfully uninstalled pinecone-plugin-inference-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pinecone 5.4.2 requires pinecone-plugin-inference<4.0.0,>=2.0.0, but you have pinecone-plugin-inference 1.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pinecone-plugin-inference-1.1.0\n",
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.10/dist-packages (5.4.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2024.12.14)\n",
            "Collecting pinecone-plugin-inference<4.0.0,>=2.0.0 (from pinecone)\n",
            "  Using cached pinecone_plugin_inference-3.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Using cached pinecone_plugin_inference-3.1.0-py3-none-any.whl (87 kB)\n",
            "Installing collected packages: pinecone-plugin-inference\n",
            "  Attempting uninstall: pinecone-plugin-inference\n",
            "    Found existing installation: pinecone-plugin-inference 1.1.0\n",
            "    Uninstalling pinecone-plugin-inference-1.1.0:\n",
            "      Successfully uninstalled pinecone-plugin-inference-1.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pinecone-client 5.0.1 requires pinecone-plugin-inference<2.0.0,>=1.0.3, but you have pinecone-plugin-inference 3.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pinecone-plugin-inference-3.1.0\n"
          ]
        }
      ],
      "source": [
        "! pip install openai\n",
        "! pip install pinecone-client\n",
        "! pip install pinecone"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k8Z2lZTuExul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pinecone\n",
        "from openai import OpenAI\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "ftuqZKb3vdF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)"
      ],
      "metadata": {
        "id": "gd8wbmvMvhlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENROUTER_API = userdata.get('OPENROUTER_API_KEY')\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=OPENROUTER_API,\n",
        ")"
      ],
      "metadata": {
        "id": "Fsne9eeY13-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"business-qa\""
      ],
      "metadata": {
        "id": "GyNJmJ2Yzp2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if index_name not in pc.list_indexes():\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=768,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(\n",
        "              cloud=\"aws\",\n",
        "              region=\"us-east-1\"\n",
        "        ),\n",
        "        deletion_protection=\"disabled\"\n",
        "    )\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IqpJ4W5NztJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc.list_indexes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14MouAn3Kpd4",
        "outputId": "93e6cef7-2fc7-44fa-9b04-db31dff73dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\n",
              "    {\n",
              "        \"name\": \"business-qa\",\n",
              "        \"dimension\": 768,\n",
              "        \"metric\": \"cosine\",\n",
              "        \"host\": \"business-qa-z9js94f.svc.aped-4627-b74a.pinecone.io\",\n",
              "        \"spec\": {\n",
              "            \"serverless\": {\n",
              "                \"cloud\": \"aws\",\n",
              "                \"region\": \"us-east-1\"\n",
              "            }\n",
              "        },\n",
              "        \"status\": {\n",
              "            \"ready\": true,\n",
              "            \"state\": \"Ready\"\n",
              "        },\n",
              "        \"deletion_protection\": \"disabled\"\n",
              "    }\n",
              "]"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "p539erg9P2UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "using distilbert model for embeddings"
      ],
      "metadata": {
        "id": "rVVzuPVmdQHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = BertModel.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1Hwp_ZQP9aj",
        "outputId": "69e5a06e-be12-469f-f31d-d7b8416327f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n",
            "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_text(text: str) -> list:\n",
        "    \"\"\"\n",
        "    Embed a given text using the pre-trained BERT model, to upload to pinecone.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "DR6tDcwC35gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_txt_file(file_path: str) -> list:\n",
        "    \"\"\"\n",
        "    Read a .txt file and split its content into smaller chunks. This file will contain info about AMD\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    words = text.split()\n",
        "    chunk_size = 500\n",
        "    chunks = [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "67-CPR0w4CGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_txt_to_pinecone(file_path: str, index):\n",
        "    \"\"\"\n",
        "    Process and upload the .txt file to Pinecone as embeddings.\n",
        "    \"\"\"\n",
        "    chunks = read_txt_file(file_path)\n",
        "    print(f\"Processing {len(chunks)} chunks from {file_path}\")\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        embedding = embed_text(chunk)\n",
        "        index.upsert([\n",
        "            {\n",
        "                \"id\": f\"{os.path.basename(file_path)}_chunk_{i}\",\n",
        "                \"values\": embedding,\n",
        "                \"metadata\": {\"content\": chunk}\n",
        "            }\n",
        "        ])\n",
        "    print(f\"Uploaded {len(chunks)} chunks from {file_path} to Pinecone.\")"
      ],
      "metadata": {
        "id": "Cwi0PNj94Nhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_relevant_documents(query: str, top_k: int = 5) -> list:\n",
        "    \"\"\"\n",
        "    Retrieve relevant documents (AMD info file) from Pinecone for the given query.\n",
        "    \"\"\"\n",
        "    query_vector = embed_text(query)\n",
        "    results = index.query(\n",
        "        vector=query_vector,\n",
        "        top_k=top_k,\n",
        "        include_metadata=True\n",
        "    )\n",
        "    return [match[\"metadata\"][\"content\"] for match in results[\"matches\"]]"
      ],
      "metadata": {
        "id": "sW2Rz8veGXkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(query: str, documents: list) -> str:\n",
        "    \"\"\"\n",
        "    Generate a response based on the retrieved document from pinecone.\n",
        "    \"\"\"\n",
        "    context = \"\\n\".join(documents)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant for answering questions about business in AMD.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Context: {context}\\n\\nQuery: {query}\"\n",
        "        }\n",
        "    ]\n",
        "    completion = client.chat.completions.create(\n",
        "        model= 'google/gemini-2.0-flash-exp:free',\n",
        "        messages=messages\n",
        "    )\n",
        "    return completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "INBdK9PxGZed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "uploading the file with information about AMD"
      ],
      "metadata": {
        "id": "OnwQ6akUelAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/amd.txt\"\n",
        "upload_txt_to_pinecone(file_path, index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGQJTenGYQ1Z",
        "outputId": "36e6f7a0-9621-4df7-c19c-3c964b21f88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 21 chunks from /content/amd.txt\n",
            "Uploaded 21 chunks from /content/amd.txt to Pinecone.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query"
      ],
      "metadata": {
        "id": "kz6w8kBae2Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"amd did partenership with which companies?\"\n",
        "relevant_docs = query_relevant_documents(user_query)"
      ],
      "metadata": {
        "id": "TjRoLIc9YX4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "retrieving response from model (testing)"
      ],
      "metadata": {
        "id": "gb15kAbvfBvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(user_query, relevant_docs)\n",
        "\n",
        "print(\"User Query:\", user_query)\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vks_bv_xGbTO",
        "outputId": "206a0800-401b-42fc-ed2b-0bd2076a69bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Query: amd did partenership with which companies?\n",
            "Response: Based on the provided text, here are the companies that AMD has partnered with:\n",
            "\n",
            "*   **Fujitsu:** For flash memory manufacturing, leading to the creation of Spansion.\n",
            "*   **Advanced Technology Investment Co. (ATIC):**  A joint venture to form GlobalFoundries for manufacturing operations.\n",
            "*   **Lenovo:** Rory Read, a former Lenovo executive, became CEO of AMD.\n",
            "*   **Hindustan Semiconductor Manufacturing Corporation (HSMC):** For the production of AMD products in India.\n",
            "*   **Sun Microsystems:** To enhance OpenSolaris and Sun xVM on the AMD platform.\n",
            "*    **Meta:** to make the chips used in the Metaverse\n",
            "*   **Samsung:** to develop a mobile processor.\n",
            "*   **Microsoft** and **Sony** to power their next-generation game consoles.\n",
            "*  **U.S. Department of Energy, Oak Ridge National Laboratory, and Cray Inc.:** to develop the Frontier exascale supercomputer\n",
            "* **U.S. Department of Energy, Lawrence Livermore National Laboratory, and HPE:** to develop the El Capitan exascale supercomputer\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating function to retrieve response"
      ],
      "metadata": {
        "id": "LpnNhVyagBLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def question (question: str):\n",
        "    '''\n",
        "    Input question from user\n",
        "    '''\n",
        "    user_query = question\n",
        "    relevant_docs = query_relevant_documents(user_query)\n",
        "    response = generate_response(user_query, relevant_docs)\n",
        "\n",
        "    print(\"User Query:\", user_query)\n",
        "    print(\"Response:\", response)"
      ],
      "metadata": {
        "id": "eimXHrkOJp2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question('What is AMD?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH-SA-MGgYuR",
        "outputId": "428d8dd0-c1c1-49e3-f81f-1f9e47d7841b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Query: What is AMD?\n",
            "Response: Based on the provided text, here's a breakdown of what AMD is:\n",
            "\n",
            "**AMD (Advanced Micro Devices) is a technology company that designs and produces a variety of hardware and software products,** primarily focused on:\n",
            "\n",
            "**1. CPUs and APUs (Central Processing Units and Accelerated Processing Units):**\n",
            "   *   AMD is a major player in the microprocessor market, producing CPUs for desktops, laptops, and servers.\n",
            "   *   They also develop APUs, which combine a CPU and a GPU (graphics processing unit) on the same chip, designed to improve performance and efficiency.\n",
            "   *   They have families like the Zen series, which is currently in its 5th generation, and previously the Bulldozer series.\n",
            "\n",
            " **2. GPUs (Graphics Processing Units):**\n",
            "   *   AMD produces graphics cards (primarily under the Radeon brand) for gaming, professional graphics, and other applications.\n",
            "   *   They also provide software tools like GPUOpen and ROCm for graphics and compute tasks.\n",
            "\n",
            " **3. Software:**\n",
            "    * AMD has made a focus of opening their software for both graphics and computing.\n",
            "    * They developed suites like, AMD Radeon Software, AOCC, and AMDuProf.\n",
            "   \n",
            "**Key aspects of AMD's business:**\n",
            "\n",
            "*   **Innovation:** AMD has a history of innovation, constantly developing new architectures and technologies for both CPUs and GPUs.\n",
            "*   **Competition:** They are a major competitor to Intel in the CPU market and Nvidia in the GPU market.\n",
            "*   **Acquisitions:** AMD has grown through strategic acquisitions of companies like ATI Technologies (for graphics) and Xilinx (for FPGAs).\n",
            "*   **Open Source:** AMD has demonstrated commitment to the open-source community with projects like GPUOpen and ROCm\n",
            "*   **Partnerships:** AMD has made significant partnerships with other tech companies such as Samsung and Meta.\n",
            "*   **Global Presence:** They have a presence in various markets, ranging from consumers to embedded systems to high-performance computing and artificial intelligence.\n",
            "\n",
            "In short, AMD is a technology company that focuses on designing and producing high-performance CPUs, GPUs, and the required supporting software, while also being a strong proponent of open-source and strategic partnerships.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question(\"Who is current CEO of AMD\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2zNFSs8jt_F",
        "outputId": "32812cb5-65bf-4a26-bfa0-cfd665776580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Query: Who is current CEO of AMD\n",
            "Response: Based on the provided text, the current CEO of AMD is **Lisa Su**.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question(\"What is Zen 2 microarchitecture?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOz2MASDksxR",
        "outputId": "5dee3710-963c-4ae2-d885-45364ffefcc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Query: What is Zen 2 microarchitecture?\n",
            "Response: Based on the provided text, here's what we know about the Zen 2 microarchitecture:\n",
            "\n",
            "**Key Features and Release:**\n",
            "\n",
            "*   **7nm Process:** Zen 2 is built using a 7nm manufacturing process. This smaller process node generally leads to improved energy efficiency and higher transistor density.\n",
            "*   **June 2019 Launch:** AMD launched CPUs based on the 7nm Zen 2 microarchitecture in June 2019. \n",
            "*   **Epyc Update:**  The Epyc line of server processors was also updated in August 2019 to use the Zen 2 microarchitecture.\n",
            "*   **Mobile Processors:** In 2020, AMD announced the Ryzen Mobile 4000 series, which was their first 7nm x86 mobile processor and the first 8-core/16-thread high-performance mobile processor. It used the Zen 2 architecture.\n",
            "*   **Gaming Consoles:** The Steam Deck, PlayStation 5, Xbox Series X, and Series S all utilize chips based on the Zen 2 microarchitecture, though with custom implementations for each system.\n",
            "\n",
            "**Key Characteristics**\n",
            "\n",
            "*   **Improvements over Zen+:**  The text implies Zen 2 offered substantial architectural improvements over the prior Zen+ microarchitecture, considering it moved to a new 7nm build process.\n",
            " \n",
            "\n",
            "**In Summary**\n",
            "\n",
            "The Zen 2 microarchitecture was a very important release for AMD. It was launched during 2019 and 2020. It represented a significant step in AMD's innovation that led to it's processors outselling Intel's desktop processors. Also, several popular game consoles use the 7nm based Zen 2 microarchitecture.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vague question"
      ],
      "metadata": {
        "id": "6pNawjbGlu2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question(\"dd\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgM6yfSHlYA2",
        "outputId": "b1b88a87-9ba3-4dcf-91f7-19509cf1acf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Query: dd\n",
            "Response: Okay, I will respond to your query with information from the provided text.\n",
            "\n",
            "You asked me to clarify the meaning of \"dd\". However, the term \"dd\" by itself is *not mentioned* in the provided text. \n",
            "\n",
            "Therefore, I cannot provide a definition for \"dd\" specifically in the context of AMD or its business, according to the text you have provided. \n",
            "\n",
            "Without additional context, \"dd\" is ambiguous and could refer to a few things. More information is needed to understand the context.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VZY3xMYxlkWV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}